{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1e57b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdbb9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = r\"C:\\Users\\HI\\Downloads\\face_mask\\dataset\\dataset\"\n",
    "categories = os.listdir(data_path)\n",
    "label_dict = dict(zip(categories, range(len(categories))))\n",
    "\n",
    "img_size = 100\n",
    "data = []\n",
    "target = []\n",
    "\n",
    "for category in categories:\n",
    "    folder_path = os.path.join(data_path, category)\n",
    "    img_names = os.listdir(folder_path)\n",
    "\n",
    "    for img_name in img_names:\n",
    "        img_path = os.path.join(folder_path, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "        try:\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            resized = cv2.resize(gray, (img_size, img_size))\n",
    "            data.append(resized)\n",
    "            target.append(label_dict[category])\n",
    "        except Exception as e:\n",
    "            print(\"Exception:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd3086c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = np.array(data) / 255.0\n",
    "data = np.reshape(data, (data.shape[0], img_size, img_size, 1))\n",
    "target = to_categorical(np.array(target))\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(data, target, test_size=0.2, stratify=target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7378f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HI\\Documents\\face_mask\\.conda\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(200, (3, 3), activation='relu', input_shape=data.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(100, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d5867c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HI\\Documents\\face_mask\\.conda\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 974ms/step - accuracy: 0.5079 - loss: 0.7569 - val_accuracy: 0.5181 - val_loss: 0.6928\n",
      "Epoch 2/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.5125 - loss: 0.7044 - val_accuracy: 0.5145 - val_loss: 0.6773\n",
      "Epoch 3/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.5846 - loss: 0.6771 - val_accuracy: 0.6486 - val_loss: 0.6106\n",
      "Epoch 4/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.6120 - loss: 0.6465 - val_accuracy: 0.8080 - val_loss: 0.4731\n",
      "Epoch 5/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.6915 - loss: 0.5849 - val_accuracy: 0.9312 - val_loss: 0.4238\n",
      "Epoch 6/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.7605 - loss: 0.5280 - val_accuracy: 0.8877 - val_loss: 0.3096\n",
      "Epoch 7/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.7987 - loss: 0.4738 - val_accuracy: 0.9239 - val_loss: 0.2851\n",
      "Epoch 8/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.7991 - loss: 0.4507 - val_accuracy: 0.8768 - val_loss: 0.2631\n",
      "Epoch 9/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.8307 - loss: 0.3986 - val_accuracy: 0.8841 - val_loss: 0.2512\n",
      "Epoch 10/10\n",
      "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.8293 - loss: 0.4126 - val_accuracy: 0.9348 - val_loss: 0.1763\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2360af03af0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "datagen.fit(xtrain)\n",
    "\n",
    "model.fit(datagen.flow(xtrain, ytrain, batch_size=32), epochs=10, validation_data=(xtest, ytest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed6f62d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 237ms/step\n",
      "Confusion Matrix:\n",
      "[[137   1]\n",
      " [ 17 121]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.99      0.94       138\n",
      "           1       0.99      0.88      0.93       138\n",
      "\n",
      "    accuracy                           0.93       276\n",
      "   macro avg       0.94      0.93      0.93       276\n",
      "weighted avg       0.94      0.93      0.93       276\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "y_pred = model.predict(xtest)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = np.argmax(ytest, axis=1)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_true, y_pred_classes))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06890f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "768dd61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model-026.keras')\n",
    "\n",
    "face_clsfr=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "source=cv2.VideoCapture(0)\n",
    "\n",
    "labels_dict={0:'MASK',1:'NO MASK'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcca6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.exists('haarcascade_frontalface_default.xml'))  # Should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "247d2a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b50980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to grab frame\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    ret, img = source.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_clsfr.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_img = gray[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(face_img, (100, 100))\n",
    "        normalized = resized / 255.0\n",
    "        reshaped = np.reshape(normalized, (1, 100, 100, 1))\n",
    "        result = model.predict(reshaped)\n",
    "\n",
    "        label = np.argmax(result, axis=1)[0]\n",
    "\n",
    "        cv2.rectangle(img, (x, y), (x+w, y+h), color_dict[label], 2)\n",
    "        cv2.rectangle(img, (x, y-40), (x+w, y), color_dict[label], -1)\n",
    "        cv2.putText(img, labels_dict[label], (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    cv2.imshow('LIVE', img)\n",
    "    key = cv2.waitKey(1)\n",
    "\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "source.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
